{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Preprocess Images\n",
    "In this notebook, we will download the images from s3 bucket, correct the orientation of the images using exif tag and upload them back to another s3 bucket for training.\n",
    "\n",
    "**We do NOT need a GPU for this notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting exif\n",
      "  Downloading exif-1.2.2-py3-none-any.whl (29 kB)\n",
      "Collecting plum-py==0.3.1\n",
      "  Downloading plum_py-0.3.1-py3-none-any.whl (69 kB)\n",
      "     |████████████████████████████████| 69 kB 12.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=39.0.1 in /opt/conda/lib/python3.6/site-packages (from plum-py==0.3.1->exif) (59.6.0)\n",
      "Installing collected packages: plum-py, exif\n",
      "Successfully installed exif-1.2.2 plum-py-0.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install exif package\n",
    "!pip install exif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd  # Home of the DataFrame construct, _the_ most important object for Data Science\n",
    "import sys  # Python system library needed to load custom functions\n",
    "import glob\n",
    "import exif\n",
    "import PIL\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import DEFAULT_BUCKET, ORIGINAL_BUCKET\n",
    "from gdsc_util import download_directory, download_file, upload_file, load_sections_df, set_up_logging, PROJECT_DIR\n",
    "set_up_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_directory('jpgs/', local_dir='data', bucket=ORIGINAL_BUCKET)  # Download the JPG images into our data folder\n",
    "download_file('gdsc_train.csv', local_dir='data', bucket=ORIGINAL_BUCKET)  # Download the list of worm sections\n",
    "download_file('test_files.csv', local_dir='data', bucket=ORIGINAL_BUCKET)  # Download the files for which we need to create a predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the the images were rotated after they were created and labelled. As it turns out, the rotation is only done via exif annotation and not at a fundamental level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the orientation of the images using exif\n",
    "img_paths = glob.glob('../data/jpgs/*.jpg')\n",
    "for img_path in img_paths:\n",
    "    img = PIL.Image.open(img_path)\n",
    "    if not img.getexif(): # No EXIF tag at all\n",
    "        continue \n",
    "    # Load Image EXIF\n",
    "    with open(img_path, 'rb') as f:\n",
    "        img_exif = exif.Image(f)\n",
    "    # Delete orientation tag and store the image \n",
    "    if 'orientation' in dir(img_exif):\n",
    "        print(img_path)\n",
    "        img_exif.delete('orientation')\n",
    "        with open(img_path, 'wb') as f:\n",
    "            f.write(img_exif.get_file())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the corrected images to another s3 bucket i.e. DEFAULT_BUCKET which we will use for training\n",
    "img_paths = glob.glob('../data/jpgs/*.jpg')\n",
    "for local_path in img_paths:\n",
    "    s3_path = 'jpgsnew/' + local_path.split('/')[-1]\n",
    "    upload_file(local_path, s3_path, DEFAULT_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the train and test csv files to DEFAULT_BUCKET\n",
    "upload_file('../data/gdsc_train.csv', 'gdsc_train.csv', DEFAULT_BUCKET)\n",
    "upload_file('../data/test_files.csv', 'test_files.csv', DEFAULT_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load gdsc_train.csv and divide it into train and validation set. We will use 90% of data for training and rest 10% for validation. For creating the validation set, we will take random 10% of data from each stain. We will then save the train and validation data in src folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stain : D, Number of files for validation : 21\n",
      "Stain : C, Number of files for validation : 8\n",
      "Stain : B, Number of files for validation : 24\n",
      "Stain : A, Number of files for validation : 42\n",
      "Stain : DD, Number of files for validation : 3\n"
     ]
    }
   ],
   "source": [
    "data = load_sections_df('../data/gdsc_train.csv')\n",
    "train = []\n",
    "val = []\n",
    "val_percent = 0.1\n",
    "for stain in data['staining'].unique():\n",
    "    df = data[data['staining']==stain]\n",
    "    np.random.seed(0)\n",
    "    filenames = df['file_name'].unique()\n",
    "    random_list = np.random.rand(len(filenames))\n",
    "    val_filenames = filenames[random_list<val_percent]\n",
    "    print(f'Stain : {stain}, Number of files for validation : {len(val_filenames)}')\n",
    "    train_filenames = filenames[random_list>=val_percent]\n",
    "    val.append(df[df['file_name'].isin(val_filenames)])\n",
    "    train.append(df[df['file_name'].isin(train_filenames)])\n",
    "train_df = pd.concat(train, ignore_index=True)\n",
    "val_df = pd.concat(val, ignore_index=True)\n",
    "train_df.to_csv(f'../src/gdsc_train_dataset_{int((1-val_percent)*100)}.csv', sep=';', index=False)\n",
    "val_df.to_csv(f'../src/gdsc_val_dataset_{int(val_percent*100)}.csv', sep=';', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3 (gdsc5-smstudio-custom/1)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:914063116219:image-version/gdsc5-smstudio-custom/1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
